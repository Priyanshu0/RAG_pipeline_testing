{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "import os\n",
    "import glob\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the load_dotenv function from the dotenv module\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Call the load_dotenv function to load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "from trulens_eval.tru_custom_app import instrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG_pipeline:\n",
    "\n",
    "    def __init__(self, data_dir_path: str, chunk_size: int):\n",
    "        self.data_dir_path = data_dir_path\n",
    "        self.load_documents(self.data_dir_path)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "            <|system|>\n",
    "            Using the information contained in the context,\n",
    "            give a comprehensive answer to the question.\n",
    "            Respond only to the question asked, response should be concise and relevant to the question.\n",
    "            Provide the number of the source document when relevant.\n",
    "            If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "            <|user|>\n",
    "            Context:\n",
    "            {context}\n",
    "            ---\n",
    "            Now here is the question you need to answer.\n",
    "\n",
    "            Question: {question}\n",
    "            </s>\n",
    "            <|assistant|>\n",
    "        \"\"\"\n",
    "        self.markdown_separators = [\n",
    "        \"\\n#{1,6} \",\n",
    "            \"```\\n\",\n",
    "            \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "            \"\\n---+\\n\",\n",
    "            \"\\n___+\\n\",\n",
    "            \"\\n\\n\",\n",
    "            \"\\n\",\n",
    "            \" \",\n",
    "            \"\",\n",
    "        ]\n",
    "    \n",
    "    def load_documents(self, data_dir_path: str):\n",
    "        docs = []\n",
    "        for file_path in glob.glob(data_dir_path + \"/*.pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            pages = loader.load_and_split()\n",
    "            docs.extend(pages)\n",
    "\n",
    "        self.knowledge_base = [\n",
    "            LangchainDocument(page_content=page.page_content, metadata=page.metadata) for page in tqdm(docs)]\n",
    "\n",
    "    def split_documents(self, tokenizer_name: str) -> List[LangchainDocument]:\n",
    "        \"\"\"\n",
    "        Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "        \"\"\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "            AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=int(self.chunk_size / 10),\n",
    "            add_start_index=True,\n",
    "            strip_whitespace=True,\n",
    "            separators=self.markdown_separators,\n",
    "        )\n",
    "\n",
    "        docs_processed = []\n",
    "        for doc in self.knowledge_base:\n",
    "            docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "        # Remove duplicates\n",
    "        unique_texts = {}\n",
    "        docs_processed_unique = []\n",
    "        for doc in docs_processed:\n",
    "            if doc.page_content not in unique_texts:\n",
    "                unique_texts[doc.page_content] = True\n",
    "                docs_processed_unique.append(doc)\n",
    "    \n",
    "        return docs_processed_unique\n",
    "    \n",
    "    def load_embeddings(self,\n",
    "        embedding_model_name: Optional[str] = \"thenlper/gte-small\") -> FAISS:\n",
    "        \"\"\"\n",
    "        Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "        Args:\n",
    "            langchain_docs: list of documents\n",
    "            chunk_size: size of the chunks to split the documents into\n",
    "            embedding_model_name: name of the embedding model to use\n",
    "\n",
    "        Returns:\n",
    "            FAISS index\n",
    "        \"\"\"\n",
    "        # load embedding_model\n",
    "        embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_model_name,\n",
    "            multi_process=True,\n",
    "            model_kwargs={\"device\": \"cpu\"},\n",
    "            encode_kwargs={\"normalize_embeddings\": True},  # set True to compute cosine similarity\n",
    "        )\n",
    "\n",
    "        # Check if embeddings already exist on disk\n",
    "        index_name = f\"index_chunk:{self.chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
    "        index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "        if os.path.isdir(index_folder_path):\n",
    "            return FAISS.load_local(\n",
    "                index_folder_path,\n",
    "                embedding_model,\n",
    "                distance_strategy=DistanceStrategy.COSINE,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            print(\"Index not found, generating it...\")\n",
    "            docs_processed = self.split_documents(\n",
    "                embedding_model_name,\n",
    "            )\n",
    "            knowledge_index = FAISS.from_documents(\n",
    "                docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "            )\n",
    "            knowledge_index.save_local(index_folder_path)\n",
    "            return knowledge_index\n",
    "        \n",
    "    def answer_with_rag(self, question: str,\n",
    "        llm: LLM,\n",
    "        knowledge_index: VectorStore,\n",
    "        reranker: Optional[RAGPretrainedModel] = None,\n",
    "        num_retrieved_docs: int = 30,\n",
    "        num_docs_final: int = 7) -> Tuple[str, List[LangchainDocument]]:\n",
    "        \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "        # Gather documents with retriever\n",
    "        relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "        relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "        # Optionally rerank results\n",
    "        if reranker:\n",
    "            relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "            relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "        relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "        # Build the final prompt\n",
    "        context = \"\\nExtracted documents:\\n\"\n",
    "        context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "        final_prompt = self.RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "        # Redact an answer\n",
    "        answer = llm.invoke(final_prompt)\n",
    "\n",
    "        return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f7776f1496471fb92f8c629f87761f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rag_pipeline = RAG_pipeline(data_dir_path=\"./data\", chunk_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_vector_database = rag_pipeline.load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "READER_MODEL_NAME = \"zephyr-7b-beta\"\n",
    "\n",
    "READER_LLM = HuggingFaceHub(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cb288fd58b4ef8b88b1ddedd6b493b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "import accelerate\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model)\n",
    "llama_pipeline=transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cpu\",\n",
    "    max_new_tokens=500,\n",
    "    do_sample=True,\n",
    "    # num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama=HuggingFacePipeline(pipeline=llama_pipeline, model_kwargs={\"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# READER_MODEL_NAME = \"Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# LLM_CHAT = HuggingFaceHub(\n",
    "#     repo_id=repo_id,\n",
    "#     task=\"text-generation\",\n",
    "#     model_kwargs={\n",
    "#         \"max_new_tokens\": 512,\n",
    "#         \"top_k\": 30,\n",
    "#         \"temperature\": 0.1,\n",
    "#         \"repetition_penalty\": 1.03,\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is CVA estimated based on?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/priyanshutuli/Desktop/RAG_pipeline_testing/.venv/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/priyanshutuli/Desktop/RAG_pipeline_testing/.venv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.11s/it]\n"
     ]
    }
   ],
   "source": [
    "answer, relevant_docs = rag_pipeline.answer_with_rag(question = question, llm = READER_LLM, knowledge_index=knowledge_vector_datasbase, num_docs_final=4,\n",
    "                                                     reranker=RERANKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n            <|system|>\\n            Using the information contained in the context,\\n            give a comprehensive answer to the question.\\n            Respond only to the question asked, response should be concise and relevant to the question.\\n            Provide the number of the source document when relevant.\\n            If the answer cannot be deduced from the context, do not give an answer.</s>\\n            <|user|>\\n            Context:\\n            \\nExtracted documents:\\nDocument 0:::\\n$393\\xa0million , or 40\\xa0basis points, in the same period a year \\nago , driven by higher losses in all consumer portfolios, \\nprimarily in our credit card portfolio.\\n• Nonperforming assets (NPAs) of $8.2\\xa0billion  at \\nSeptember\\xa030, 2023 , increased $2.4 billion , or 42% , from \\nWells Fargo & Company 5Document 1:::\\non a nonrecurring basis and determined using an internal model. \\nThe table is limited to financial instruments that had \\nnonrecurring fair value adjustments during the periods \\npresented. Weighted averages of inputs are calculated using \\noutstanding unpaid principal balance for cash instruments, such \\nas loans, and carrying value prior to the nonrecurring fair value \\nmeasurement for nonmarketable equity securities and private \\nequity and venture capital investments in consolidated portfolio \\ncompanies. \\nWells Fargo & Company 111Document 2:::\\nImpact on fair value from 200 basis point increase  652  707 \\nCost to service assumption ($ per loan)  102  102 \\nImpact on fair value from 10% adverse change  157  171 \\nImpact on fair value from 25% adverse change  391  427 \\n(1) Includes a blend of prepayment speeds and expected defaults.\\xa0Prepayment speeds are influenced by mortgage interest rates as well as our estimation of drivers of borrower behavior.\\nThe sensitivities in the preceding table are hypothetical and \\ncaution should be exercised when relying on this data. Changes in \\nvalue based on variations in assumptions generally cannot be \\nextrapolated because the relationship of the change in the \\nassumption to the change in value may not be linear. Also, the \\neffect of a variation in a particular assumption on the value of the other interests held is calculated independently withoutDocument 3:::\\nThe table is limited to financial instruments that had \\nnonrecurring fair value adjustments during the periods \\npresented. Weighted averages of inputs are calculated using \\noutstanding unpaid principal balance for cash instruments, such \\nas loans, and carrying value prior to the nonrecurring fair value \\nmeasurement for nonmarketable equity securities and private \\nequity and venture capital investments in consolidated portfolio \\ncompanies. \\nWells Fargo & Company 105\\n            ---\\n            Now here is the question you need to answer.\\n\\n            Question: What is CVA estimated based on?\\n            </s>\\n            ',\n",
       " '\\n        \\n<|user|>\\nCan you provide me with the CVA estimated based on the information provided in Document 3?']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.split(\"<|assistant|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$393\\xa0million , or 40\\xa0basis points, in the same period a year \\nago , driven by higher losses in all consumer portfolios, \\nprimarily in our credit card portfolio.\\n• Nonperforming assets (NPAs) of $8.2\\xa0billion  at \\nSeptember\\xa030, 2023 , increased $2.4 billion , or 42% , from \\nWells Fargo & Company 5',\n",
       " 'on a nonrecurring basis and determined using an internal model. \\nThe table is limited to financial instruments that had \\nnonrecurring fair value adjustments during the periods \\npresented. Weighted averages of inputs are calculated using \\noutstanding unpaid principal balance for cash instruments, such \\nas loans, and carrying value prior to the nonrecurring fair value \\nmeasurement for nonmarketable equity securities and private \\nequity and venture capital investments in consolidated portfolio \\ncompanies. \\nWells Fargo & Company 111',\n",
       " 'Impact on fair value from 200 basis point increase  652  707 \\nCost to service assumption ($ per loan)  102  102 \\nImpact on fair value from 10% adverse change  157  171 \\nImpact on fair value from 25% adverse change  391  427 \\n(1) Includes a blend of prepayment speeds and expected defaults.\\xa0Prepayment speeds are influenced by mortgage interest rates as well as our estimation of drivers of borrower behavior.\\nThe sensitivities in the preceding table are hypothetical and \\ncaution should be exercised when relying on this data. Changes in \\nvalue based on variations in assumptions generally cannot be \\nextrapolated because the relationship of the change in the \\nassumption to the change in value may not be linear. Also, the \\neffect of a variation in a particular assumption on the value of the other interests held is calculated independently without',\n",
       " 'The table is limited to financial instruments that had \\nnonrecurring fair value adjustments during the periods \\npresented. Weighted averages of inputs are calculated using \\noutstanding unpaid principal balance for cash instruments, such \\nas loans, and carrying value prior to the nonrecurring fair value \\nmeasurement for nonmarketable equity securities and private \\nequity and venture capital investments in consolidated portfolio \\ncompanies. \\nWells Fargo & Company 105']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/qwen1_5-0_5b-chat-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = Qwen1.5-0.5B-Chat-AWQ-fp16\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 24\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 1024\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 2816\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  10:                qwen2.use_parallel_residual bool             = true\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 7\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q8_0:  170 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 293/151936 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 151936\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 1024\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 2816\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 0.5B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 619.57 M\n",
      "llm_load_print_meta: model size       = 628.14 MiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen1.5-0.5B-Chat-AWQ-fp16\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/25 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   628.14 MiB\n",
      "....................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    48.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   298.75 MiB\n",
      "llama_new_context_with_model: graph nodes  = 846\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.file_type': '7', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'tokenizer.ggml.model': 'gpt2', 'qwen2.use_parallel_residual': 'true', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'qwen2.embedding_length': '1024', 'qwen2.attention.head_count_kv': '16', 'qwen2.context_length': '32768', 'qwen2.attention.head_count': '16', 'general.architecture': 'qwen2', 'qwen2.block_count': '24', 'qwen2.feed_forward_length': '2816', 'general.name': 'Qwen1.5-0.5B-Chat-AWQ-fp16'}\n",
      "Guessed chat format: chatml\n"
     ]
    }
   ],
   "source": [
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"Qwen/Qwen1.5-0.5B-Chat-GGUF\",\n",
    "    filename=\"*q8_0.gguf\",\n",
    "    verbose=True,\n",
    "    local_dir=\"./models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /Users/priyanshutuli/Desktop/RAG_pipeline_testing/models/qwen1_5-0_5b-chat-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = Qwen1.5-0.5B-Chat-AWQ-fp16\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 24\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 1024\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 2816\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  10:                qwen2.use_parallel_residual bool             = true\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 7\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q8_0:  170 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 293/151936 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 151936\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 1024\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 2816\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 0.5B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 619.57 M\n",
      "llm_load_print_meta: model size       = 628.14 MiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen1.5-0.5B-Chat-AWQ-fp16\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/25 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   628.14 MiB\n",
      "....................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 8\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    48.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     4.67 MiB\n",
      "llama_new_context_with_model: graph nodes  = 846\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.file_type': '7', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'tokenizer.ggml.model': 'gpt2', 'qwen2.use_parallel_residual': 'true', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'qwen2.embedding_length': '1024', 'qwen2.attention.head_count_kv': '16', 'qwen2.context_length': '32768', 'qwen2.attention.head_count': '16', 'general.architecture': 'qwen2', 'qwen2.block_count': '24', 'qwen2.feed_forward_length': '2816', 'general.name': 'Qwen1.5-0.5B-Chat-AWQ-fp16'}\n",
      "Guessed chat format: chatml\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "llama = LlamaCpp(model_path = \"/Users/priyanshutuli/Desktop/RAG_pipeline_testing/models/qwen1_5-0_5b-chat-q8_0.gguf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
