{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "from datasets import Dataset, Features, Sequence, Value\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_similarity,\n",
    "  \n",
    ")\n",
    "from ragas import evaluate \n",
    "import json\n",
    "import datasets\n",
    "import os\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the load_dotenv function from the dotenv module\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Call the load_dotenv function to load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG_pipeline:\n",
    "\n",
    "    def __init__(self, data_dir_path: str, chunk_size: int):\n",
    "        self.data_dir_path = data_dir_path\n",
    "        self.load_documents(self.data_dir_path)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "            <|system|>\n",
    "            Using the information contained in the context,\n",
    "            give a comprehensive answer to the question.\n",
    "            Respond only to the question asked, response should be concise and relevant to the question.\n",
    "            Provide the number of the source document when relevant.\n",
    "            If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "            <|user|>\n",
    "            Context:\n",
    "            {context}\n",
    "            ---\n",
    "            Now here is the question you need to answer.\n",
    "\n",
    "            Question: {question}\n",
    "            </s>\n",
    "            <|assistant|>\n",
    "        \"\"\"\n",
    "    \n",
    "    def load_documents(self, data_dir_path: str):\n",
    "        docs = []\n",
    "        for file_path in glob.glob(data_dir_path + \"/*.pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            pages = loader.load_and_split()\n",
    "            docs.extend(pages)\n",
    "\n",
    "        self.knowledge_base = [\n",
    "            LangchainDocument(page_content=page.page_content, metadata=page.metadata) for page in tqdm(docs)]\n",
    "\n",
    "    def split_documents(self, tokenizer_name: str) -> List[LangchainDocument]:\n",
    "        \"\"\"\n",
    "        Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "        \"\"\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "            AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=int(self.chunk_size / 10),\n",
    "            add_start_index=True,\n",
    "            strip_whitespace=True,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "        )\n",
    "\n",
    "        docs_processed = []\n",
    "        for doc in self.knowledge_base:\n",
    "            docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "        # Remove duplicates\n",
    "        unique_texts = {}\n",
    "        docs_processed_unique = []\n",
    "        for doc in docs_processed:\n",
    "            if doc.page_content not in unique_texts:\n",
    "                unique_texts[doc.page_content] = True\n",
    "                docs_processed_unique.append(doc)\n",
    "    \n",
    "        return docs_processed_unique\n",
    "    \n",
    "    def load_embeddings(self,\n",
    "        embedding_model_name: Optional[str] = \"thenlper/gte-small\") -> FAISS:\n",
    "        \"\"\"\n",
    "        Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "        Args:\n",
    "            langchain_docs: list of documents\n",
    "            chunk_size: size of the chunks to split the documents into\n",
    "            embedding_model_name: name of the embedding model to use\n",
    "\n",
    "        Returns:\n",
    "            FAISS index\n",
    "        \"\"\"\n",
    "        # load embedding_model\n",
    "        embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_model_name,\n",
    "            multi_process=True,\n",
    "            model_kwargs={\"device\": \"cpu\"},\n",
    "            encode_kwargs={\"normalize_embeddings\": True},  # set True to compute cosine similarity\n",
    "        )\n",
    "\n",
    "        # Check if embeddings already exist on disk\n",
    "        index_name = f\"index_chunk:{self.chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
    "        index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "        if os.path.isdir(index_folder_path):\n",
    "            return FAISS.load_local(\n",
    "                index_folder_path,\n",
    "                embedding_model,\n",
    "                distance_strategy=DistanceStrategy.COSINE,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            print(\"Index not found, generating it...\")\n",
    "            docs_processed = self.split_documents(\n",
    "                embedding_model_name,\n",
    "            )\n",
    "            knowledge_index = FAISS.from_documents(\n",
    "                docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "            )\n",
    "            knowledge_index.save_local(index_folder_path)\n",
    "            return knowledge_index\n",
    "        \n",
    "    def answer_with_rag(self, question: str,\n",
    "        llm: LLM,\n",
    "        knowledge_index: VectorStore,\n",
    "        reranker: Optional[RAGPretrainedModel] = None,\n",
    "        num_retrieved_docs: int = 30,\n",
    "        num_docs_final: int = 7) -> Tuple[str, List[LangchainDocument]]:\n",
    "        \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "        # Gather documents with retriever\n",
    "        relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "        relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "        # Optionally rerank results\n",
    "        if reranker:\n",
    "            relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "            relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "        relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "        # Build the final prompt\n",
    "        context = \"\\nExtracted documents:\\n\"\n",
    "        context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "        final_prompt = self.RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "        # Redact an answer\n",
    "        \n",
    "        answer = llm(final_prompt)\n",
    "\n",
    "        return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12cb2baf268c4ff3b978c12124699d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rag_pipeline = RAG_pipeline(data_dir_path=\"./data\", chunk_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index not found, generating it...\n"
     ]
    }
   ],
   "source": [
    "index = rag_pipeline.load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "READER_MODEL_NAME = \"zephyr-7b-beta\"\n",
    "\n",
    "READER_LLM = HuggingFaceHub(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure()\n",
    "model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "READER_LLM = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_ques = pd.read_csv(\"/Users/rahulkushwaha/Desktop/LLM Eval/synthethic_que/Manual QA - Q1.csv\")\n",
    "synthetic_ques2 = pd.read_csv(\"/Users/rahulkushwaha/Desktop/LLM Eval/synthethic_que/Manual QA - Q2.csv\")\n",
    "synthetic_ques3 = pd.read_csv(\"/Users/rahulkushwaha/Desktop/LLM Eval/synthethic_que/Manual QA - Q3.csv\")\n",
    "synthetic_ques4 = pd.read_csv(\"/Users/rahulkushwaha/Desktop/LLM Eval/synthethic_que/Manual QA - Q4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ans_context(df,READER_LLM,index):\n",
    "    df[\"answer\"] = None\n",
    "    df[\"contexts\"] = None\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        response,con = rag_pipeline.answer_with_rag(question = df.loc[i, 'question'], llm= READER_LLM, knowledge_index=index)\n",
    "        df.loc[i, 'answer'] = response\n",
    "        df.loc[i, 'contexts'] = con[0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_ans_context(synthetic_ques4,READER_LLM,index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAGAs_eval(df):\n",
    "    df['contexts'] = df['contexts'].apply(lambda x: x if isinstance(x, list) else [str(x)])\n",
    "    features = Features({\n",
    "        'contexts': Sequence(Value('string')), })\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    score = evaluate(dataset,metrics=[context_recall,\n",
    "    context_precision,\n",
    "    answer_similarity])\n",
    "    df = score.to_pandas()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f574dc9301e443bbd1e756cb2c75cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = RAGAs_eval(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huggingface_eval(df):\n",
    "   import evaluate\n",
    "   \n",
    "   df[\"honesty\"]=None\n",
    "   df[\"toxicity\"]=None\n",
    "   df[\"language polarity\"]=None\n",
    "\n",
    "   honest = evaluate.load('honest', 'en')\n",
    "   toxicity = evaluate.load(\"toxicity\", module_type=\"measurement\")\n",
    "   regard = evaluate.load(\"regard\")\n",
    "\n",
    "   for i in range(len(df)): \n",
    "      completions = [[df.iloc[i][\"answer\"]], [df.iloc[i][\"answer\"]]] \n",
    "      groups = [\"male\", \"female\"]  \n",
    "      honest_result = honest.compute(predictions=completions, groups=groups)  \n",
    "      df.at[i, 'honesty'] = honest_result['honest_score_per_group']\n",
    "\n",
    "      toxic_results = toxicity.compute(predictions=[df.iloc[i][\"answer\"]])\n",
    "      df.at[i, 'toxicity'] = toxic_results['toxicity']\n",
    "\n",
    "      lp_results = regard.compute(data = [df.iloc[i][\"answer\"]])\n",
    "      df.at[i, 'language polarity'] = lp_results['regard']\n",
    "    \n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n"
     ]
    }
   ],
   "source": [
    "df = huggingface_eval(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
